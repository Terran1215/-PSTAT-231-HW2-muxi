---
title: "PSTAT 231 HW2 muxi"
author: "muxi"
date: "2022-10-22"
output: html_document
---


## PSTAT 231 Homework 2

## Question 1

```{r echo=TRUE}
library(tidyverse)
library(tidymodels)
data=read.csv("abalone.csv")
head(data)
data=mutate(data,age=rings+1.5)
summary(data$age)
hist(data$age,xlab="age",main="Hist of age",col="lightblue")
```

To begin with, I believe that age could be treated as quantitative predictor. Though rings are always integers, we could use the raw data as a estimate of the exact age.

From summary and hist graph, we could see that the age is right skewed and there is no obvious outlier.

## Question 2

```{r echo=TRUE}
set.seed(1215)
data_split = initial_split(data, prop = 0.80)
data_train = training(data_split)
data_test = testing(data_split)
```

## Question 3

As age and rings are strongly positive correlated( age = rings + 1.5), the residuals plot would be a level line through residuals=0. This will remove error term, lead to overfitting and make any other predictors meaningless.

```{r echo=TRUE}
#drop rings column
train=select(data_train,-c(rings))
test=select(data_test,-c(rings))
simple_data_recipe=recipe(age ~ ., data = train)
summary(simple_data_recipe)
```

```{r echo=TRUE}
data_recipe = recipe(age~ ., data = train)
recipe=data_recipe%>% 
  step_dummy(all_nominal_predictors())%>% 
  step_interact(terms = ~ starts_with("type"):shucked_weight)%>% 
  step_interact(terms = ~ longest_shell:diameter)%>% 
  step_interact(terms = ~ shucked_weight:shell_weight)%>% 
  step_center(all_nominal_predictors())%>% 
  step_scale(all_nominal_predictors())
```

## Question 4

```{r echo=TRUE}
lm_model = linear_reg() %>% 
  set_engine("lm")

```

## Question 5

```{r echo=TRUE}
lm_wflow = workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(recipe)
lm_fit = fit(lm_wflow, train)
summary(lm_fit)
```


## Question 6

```{r echo=TRUE}

pre=train[1,]
pre[2:8]=c(0.5,0.1,0.3,4,1,2,1)
predict(lm_fit, pre)
        
```

## Question 7

```{r echo=TRUE}
library(yardstick)
train_res = predict(lm_fit, new_data =train %>% select(-age))
#predicted values vs the actual observed ages
train_res = bind_cols(train_res, train %>% select(age))
train_res %>% 
  head()
#R2, RMSE, and MAE
metrics = metric_set(rmse, rsq, mae)
metrics(train_res, truth = age, estimate = .pred)
```

```{r echo=TRUE}
train_res %>% 
  ggplot(aes(x = .pred, y = age)) +
  geom_point(alpha = 0.2) +
  geom_abline(lty = 2) + 
  theme_bw() +
  coord_obs_pred()
```

From R-square and plot, we could see that the model didnâ€™t do very well. If it predicted every observation accurately, the dots would form a straight line.  Perhaps in the future, I will try other models and other interaction methods dealing with type and shucked_weight.

## Question 8

Reproducible errors are $Var(\hat{f}(x_{0}))$, $[Bias(\hat{f}(x_{0}))]^2$.

Irreducible error is $Var(\epsilon)$.

## Question 9

$\because Var(\hat{f}(x_{0}))>0,[Bias(\hat{f}(x_{0}))]^2>0$

$\therefore E[(y_{0}-\hat{f}(x_{0}))^2]\geq Var(\epsilon)$

## Question 10

$E[(y_{0}-\hat{f}(x_{0}))^2]=E[y_{0}^2]-2E[y_{0}]E[\hat{f}(x_{0})]+E[\hat{f}(x_{0})^2]\\=Var(\epsilon)+E[y_{0}]^2-2E[y_{0}]E[\hat{f}(x_{0})]+E[\hat{f}(x_{0})]^2-E[\hat{f}(x_{0})]^2+E[\hat{f}(x_{0})^2]\\=Var(\epsilon)+(E[\hat{f}(x_{0})]-y_{0})^2+Var(\hat{f}(x_{0}))\\=Var(\epsilon)+[Bias(\hat{f}(x_{0}))]^2+Var(\hat{f}(x_{0}))$


